{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gaussian approximation potential (GAP) for  the Zundel cation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The present notebook is meant to give you an overview of the main ingredients that you need to build an interatomic potential with `librascal` and use it in connection with i-Pi (https://github.com/cosmo-epfl/i-pi) to generate molecular dynamics (MD) trajectories of the system of interest. \n",
    "We will start from building a GAP model for Zundel cations ($H_5O_2+$), using a training set obtained via Bowman PES sampling, calculate its RMSE on a test set to check its performance and run a short NVT simulation at $\\text{T} = 250\\,\\text{K}$. \n",
    "\n",
    "The mathematical framework that we are going to use is the kernel-GAP fitting method, using both total energies and atomic forces as target properties. Basically the GAP-model total energy of a zundel molecule is computed using the following expression: \n",
    "\n",
    "$$\n",
    "                    E = E_0 + \\sum_i \\sum_{s=1}^M \\alpha_s K(\\bf{d_i} , \\bf{d_s} )                   \n",
    "$$\n",
    "\n",
    "where $E_0$ represents an energy baseline (given usually by the sum of atomic self-contributions), $\\bf{d_i}$ ($i = 1, \\dots, N_{atoms}$) are the set of (normalized SOAP) descriptors describing an environment centred around atom $i$, $\\bf{d_s}$ the set of descriptors corresponding to the environments of the sparse set (of size $M$) and K a kernel matrix that describes the similarity of two different atomic environments. In our application, the kernel is just the dot product, raised to some integer power $\\zeta$: \n",
    "\n",
    "$$\n",
    "        K(\\bf{d_i} , \\bf{d_s} ) \\propto \\left| \\bf{d_i} \\cdot \\bf{d_s} \\right|^{\\zeta}           \n",
    "$$\n",
    "\n",
    "Finally $\\alpha_s$ represents the weights of each sparse environment, to be determined using Kernel-Ridge Regression (KRR). For extensive details on the SOAP GAP-model fitting procedure and interesting physical applications, we invite the reader to refer to the book chapter [1].\n",
    "Details on the implementation in `librascal` are instead given in the companion publication [2].\n",
    "\n",
    "[1]: M. Ceriotti, M.J. Willatt, and G. Csányi, in Handbook of Materials Modeling (Springer International Publishing, Cham, 2018), pp. 1–27. https://doi.org/10.1007/978-3-319-42913-7_68-1\n",
    "\n",
    "[2]: F. Musil, M. Veit, A. Goscinski, G. Fraux, M.J. Willatt, M. Stricker, T. Junge, and M. Ceriotti, J. Chem. Phys. 154, 114109 (2021). https://doi.org/10.1063/5.0044689"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to fit a potential with `librascal` (the model evaluator) and interface it with i-Pi (the MD engine) we first need to have both softwares available and correctly installed. For `librascal`, the following should suffice:\n",
    "```bash\n",
    "$ pip install git+https://github.com/cosmo-epfl/librascal.git\n",
    "```\n",
    "\n",
    "and the same for i-PI:\n",
    "```bash\n",
    "$ pip install git+https://github.com/cosmo-epfl/i-pi.git\n",
    "```\n",
    "If these steps don't work, try looking at the `README.rst` files of the respective repositories for further instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the necessary Librascal modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start from the import of all the necessary modules and classes. In `librascal`, these are: \n",
    "\n",
    "1) the SOAP descriptors of a structure, by means of the `SphericalInvariants` class;\n",
    "\n",
    "2) the kernels between a set of environments and the sparse set (the `Kernel` class);\n",
    "\n",
    "3) the GAP model itself, which is saved as an object of the `KRR` class. The predict method of the same class will allow us to give predictions to new unseen structures. \n",
    "\n",
    "NOTE: it is assumed that the input format for the training set and the test set is an ASE-compatible (e.g. extended-XYZ) file, which is then converted into an array of ASE Atoms objects, each one corresponding to a specific structure. `librascal` uses these frames to compute the representations and predict the properties of new structures. In this example on zundel cations, the target energies to build the model with are reported in the `zundel_energies.txt` file, while the atomic forces are additional columns of the xyz. \n",
    "\n",
    "Alternatively, global target properties can be reported in the header line following the ASE format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "import ase\n",
    "from ase.io import read, write\n",
    "from ase.build import make_supercell\n",
    "from ase.visualize import view\n",
    "import numpy as np\n",
    "# If installed -- not essential, though\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    tqdm = (lambda i, **kwargs: i)\n",
    "\n",
    "from time import time\n",
    "\n",
    "from rascal.models import gaptools, KRR\n",
    "from rascal.utils import dump_obj, load_obj\n",
    "from rascal.representations import SphericalInvariants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd i-PI/zundel/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the snippets below we extract the relevant properties for each ASE frame. We load the total potential energies and we use the `ASE.Atoms.arrays` methods to get the atomic forces. For more information on how to use ASE-related methods, check the [ASE Atoms documentation](https://wiki.fysik.dtu.dk/ase/ase/atoms.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first N structures of the zundel dataset\n",
    "N_dataset = 1000\n",
    "frames = read('zundel_dataset.xyz', index=':{}'.format(N_dataset))\n",
    "energies = np.loadtxt('zundel_energies.txt')[:N_dataset] \n",
    "\n",
    "#Keys of the arrays dictionary\n",
    "print(frames[0].arrays.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def extract_forces(frames,array_key='zeros'):\n",
    "    f = []\n",
    "    for frame in frames:\n",
    "        if array_key is None:\n",
    "            pass\n",
    "        elif array_key == 'zeros':\n",
    "            f.append(np.zeros(frame.get_positions().shape))\n",
    "        else:\n",
    "            f.append(frame.get_array(array_key))\n",
    "    try:\n",
    "        f = np.concatenate(f)\n",
    "    except:\n",
    "        pass\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forces = extract_forces(frames, 'forces')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the GAP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preliminary step towards building a GAP model is defining a training and a test set. The training set is built by random selection of $800$ of the available structures, while the remaining $200$ will represent the test set. At this point, we also extract the distinct atomic species present in our system, define the atomic baseline energy and extract the information regarding energies and forces in the train and test set structures, which we will need later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(ids, n_train, frames_all, y_all, f_all):\n",
    "    \"\"\"Perform a train-test split\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    ids: list(int), size N\n",
    "        Ordering (e.g. random shuffle) for selecting the split\n",
    "    n_train: int\n",
    "        Number of structures in the training set\n",
    "    frames_all: list(ase.Atoms), length N\n",
    "        List of all frames in the dataset\n",
    "    y_all: list(float) or 1-D array, length N\n",
    "        Set of all function values (energies) in the dataset\n",
    "    f_all: list or 3-D array, size (N, M, 3)\n",
    "        Set of all forces (negative gradients) in the dataset\n",
    "        M is the maximum number of atoms in any one structure\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    ((list(Atoms): frames_train, 1-D array: y_train, 3-D array: f_train),\n",
    "     (list(Atoms): frames_test, 1-D array: y_test, 3-D array: f_test))\n",
    "    \"\"\"\n",
    "    train_ids = ids[:n]\n",
    "    test_ids = ids[n:]\n",
    "\n",
    "    frames_train = [frames_all[ii] for ii in train_ids]\n",
    "    frames_test = [frames_all[ii] for ii in test_ids]\n",
    "    y_train = np.array(y_all)[train_ids]\n",
    "    y_test = np.array(y_all)[test_ids]\n",
    "    f_train = np.array(f_all).reshape(len(ids), 7, 3)[train_ids].reshape(-1, 3)\n",
    "    f_test = np.array(f_all).reshape(len(ids), 7, 3)[test_ids].reshape(-1, 3)\n",
    "    return ((frames_train, y_train, f_train), (frames_test, y_test, f_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Number of structures to train the model with\n",
    "n = 800\n",
    "\n",
    "\n",
    "# Select randomly n structures for training the model\n",
    "ids = list(range(N_dataset))\n",
    "np.random.seed(10)\n",
    "np.random.shuffle(ids)\n",
    "f = extract_forces(frames, 'forces')\n",
    "((frames_train, y_train, f_train),\n",
    " (frames_test, y_test, f_test)) = train_test_split(ids, n, frames, energies, forces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define an \"atomic energy baseline\", which defines a constant contribution for each atom type in order to shift the mean of the potential energy function close to zero and make it easier to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atomic energy baseline\n",
    "global_species = set()\n",
    "for frame in frames:\n",
    "    global_species.update(frame.get_atomic_numbers())\n",
    "global_species = np.array(list(global_species))\n",
    "\n",
    "atom_energy_baseline = np.mean(energies)/(frames[0].get_global_number_of_atoms())\n",
    "energy_baseline = {int(species): atom_energy_baseline for species in global_species}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proceed with the actual calculation of the SOAP vectors of our training set. We need to specify an hyperparameters dictionary, which `librascal` uses to compute the structural features. The meaning of each hyperparameter and how to correctly set them is reported in [the `SphericalInvariants` documentation](https://cosmo-epfl.github.io/librascal/reference/python.html#rascal.representations.SphericalInvariants). These hyperparameters can be used as default values, but a careful optimization of the interaction cutoff might be required in the case the material under investigation might present some mid- or long-range order. \n",
    "\n",
    "For the actual calculation of the SOAP features, we first create an object of the `SphericalInvariants` class, defined by its hyperparameters. The methods that we then need to use are `transform()}`, which yields a second object called the `manager` containing the representation, while `get_features()` converts it into an $NxM$ matrix, $N$ being the number of atomic environments in the training set and M the number of features per each environment.\n",
    "\n",
    "Finally, note that we first request the calculation of descriptors _without_ gradients (`compute_gradients=False`) to avoid using too much memory.  We will create a version _with_ gradients later on when we need to make a model that computes forces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters of the spherical expansion\n",
    "hypers = dict(soap_type=\"PowerSpectrum\",\n",
    "              interaction_cutoff=3.0, \n",
    "              max_radial=8, \n",
    "              max_angular=6, \n",
    "              gaussian_sigma_constant=0.5,\n",
    "              gaussian_sigma_type=\"Constant\",\n",
    "              cutoff_function_type=\"RadialScaling\",\n",
    "              cutoff_smooth_width=0.5,\n",
    "              cutoff_function_parameters=\n",
    "                    dict(\n",
    "                            rate=1,\n",
    "                            scale=3.5,\n",
    "                            exponent=4\n",
    "                        ),\n",
    "              radial_basis=\"GTO\",\n",
    "              normalize=True,\n",
    "              optimization=\n",
    "                    dict(\n",
    "                            Spline=dict(\n",
    "                               accuracy=1.0e-05\n",
    "                            )\n",
    "                        ),\n",
    "              compute_gradients=False\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet computes the descriptors and the managers for each frame, which will represent the expensive part of the calculation, in terms of memory usage.\n",
    "\n",
    "NOTE: the `librascal` structure manager only works if the atoms have been wrapped within the cell provided in the input file using e.g. `ase.Atoms.wrap()` (which is automatically called if `auto_wrap==True`, as below) or one of the structure preprocessors provided in `rascal.neighbourlist.structure_manager` module (useful especially for non-periodic structures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "soap, feature_list = gaptools.calculate_representation(frames, hypers, auto_wrap=True)\n",
    "print (\"Execution: \", time()-start, \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can also define the **sparse set**, i.e. the set of (ideally) \"maximally diverse\" environments that will be used as basis points of the kernel ridge regression (KRR, mostly equivalent to GAP) model. We can choose the number of sparse environments in `librascal` per each atomic species, by defining a dictionary containing (atomic number, number of environments) pairs. The CURFilter class then selects out the most \"representative\" environments according to the representation that we provide as the input object and by applying the CUR decomposition [3].\n",
    "\n",
    "[3]: 1 M.W. Mahoney and P. Drineas, PNAS 106, 697 (2009). https://doi.org/10.1073/pnas.0803205106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sparse = {1:250, 8:125}\n",
    "X_sparse = gaptools.sparsify_environments(soap, feature_list, n_sparse, selection_type=\"CUR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in eq. $(1)$, in order to fit a GAP potential we need to compute the kernels of all the training structure descriptors and the sparse set, as well as the gradients w.r.t. the atomic positions, if we wish to fit the atomic forces. We do so by using the `compute_kernels` utility of `gaptools`; note that this can also take some time.  (Note: Yes, this does need a version of the representation that can compute gradients, but this is handled internally; the representation gradients are calculated one by one for each structure in order to avoid using too much memory.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "(k_obj, K_sparse_sparse, K_full_sparse, K_grad_full_sparse) = gaptools.compute_kernels(\n",
    "    soap,\n",
    "    feature_list,\n",
    "    X_sparse,\n",
    "    soap_power=2\n",
    ")\n",
    "print(\"Kernel compute time: \", time()-start, \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"sparse-sparse\" kernel is between all the sparse environments with themselves, the \"full-sparse\" is between each structure (summed over the atoms) and the sparse envronments, and \"grad_full_sparse\" is the _gradient_ of the full-sparse kernel with respect to each atom in the respective structure (which is what we need to train with forces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_sparse_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_full_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_grad_full_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This would be a good place to save the kernels to disk if you're planning on doing anything with them afterwards!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the subset of the full-sparse kernel that corresponds only to the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_train_sparse, K_grad_train_sparse = gaptools.extract_kernel_indices(\n",
    "    ids[:n], K_full_sparse, K_grad_full_sparse, natoms=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_train_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_grad_train_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we group everything together to build the GAP model, which basically uses the kernel matrices to regress the weights $\\{\\alpha_s\\}_{s=1}^M$ on each sparse environment. The regularization parameters of the KRR (for both energies and forces) help avoid overfitting and account for small statistical variations in the training data points.  Finally, we use the per-atom energy baseline defined above to shift the energy and make sure the model is predicting about a mean close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = gaptools.fit_gap_simple(\n",
    "    frames_train,\n",
    "    K_sparse_sparse,\n",
    "    y_train,\n",
    "    K_train_sparse,\n",
    "    energy_regularizer_peratom=1E-10,\n",
    "    forces=f_train,\n",
    "    kernel_gradients_sparse=K_grad_train_sparse,\n",
    "    energy_atom_contributions=energy_baseline,\n",
    "    force_regularizer=1E-9,\n",
    "    solver=\"RKHS\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the weights to build a KRR object, which we can save as .json file for future use. For this last bit we use the `dump_obj` method (part of the `rascal.utils.io` module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KRR(weights, k_obj, X_sparse, energy_baseline,\n",
    "            description=\"GAP for the Zundel cation\")\n",
    "dump_obj('zundel_model.json', model)\n",
    "np.savetxt('Structure_indices.txt', ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a basic assessment of the model we have fitted, we need to predict the properties (energies and forces) of a test set. In the calculation of the model that we have fitted previously, we have randomly splitted the original dataset in a training set of 800 structures and a test set of another 200.  We must be careful to use the same training and testing set as defined above (under the section \"Building a GAP model\") and not re-shuffle the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_obj('zundel_model.json')\n",
    "\n",
    "# Uncomment the lines below if you are starting from this cell\n",
    "# n_test = 800\n",
    "# ids = np.loadtxt('Structure_indices.txt')\n",
    "# (_, (frames_test, y_test, f_test)) = train_test_split(ids, n_test, frames, energies, forces)\n",
    "# k_obj = model.kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compute the predictions on the test set, using the `predict()` and `predict_forces()` methods of the `KRR` class. At this point we need to compute the SOAP representation of the test set structures. It is important to stress however that this will not cause any issue regarding memory usage, because all we need is the predictions, so we can compute the managers of the test set structures one by one and calculate the predictions right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers_grad = hypers.copy()\n",
    "hypers_grad['compute_gradients'] = True\n",
    "soap_grad = SphericalInvariants(**hypers_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the test set\n",
    "y_pred = []\n",
    "f_pred = []\n",
    "\n",
    "for f in tqdm(frames_test):\n",
    "    positions = f.get_positions()\n",
    "    f.set_positions(positions+[1,1,1])\n",
    "    f.wrap(eps=1e-18)\n",
    "    m = soap_grad.transform(f)\n",
    "    y_pred.append(model.predict(m))\n",
    "    f_pred.append(model.predict_forces(m))\n",
    "\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "f_pred = np.array(f_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compute the overall performance of the model. This is done by calculation of the Root-Mean-Square Error (RMSE), i.e. the standard deviation of the residuals according to the standard formula:\n",
    "\n",
    "$$\n",
    "       \\text{RMSE} = \\sqrt{\\frac{1}{n_{\\text{test}}}\\sum_i (y_{\\text{pred}}^{i} - y_{\\text{test}}^{i})^2}\n",
    "$$\n",
    "   \n",
    "which we can compare to the standard deviation of the test set itself to quantify how much the model captures the energy variations in the test set. The $\\% \\text{RMSE}$ of our model is about $5 \\%$ of the training set STD, which is sufficiently accurate to run MD safely. \n",
    "\n",
    "Finally we plot a \"correlation plot\", to observe how well the predictions on the test set correlate with the reference DFT-computed energies.\n",
    "\n",
    "NOTE: by just repeating this procedure and fitting potentials with increasing training set size, one can also construct the learning curve of the GAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rascal.utils import get_score\n",
    "\n",
    "score = get_score(y_pred, y_test)\n",
    "RMSE = score['RMSE']\n",
    "sigma_test = np.std(y_test)\n",
    "\n",
    "score = get_score(f_test.flatten(), f_pred.flatten())\n",
    "fRMSE = score['RMSE']\n",
    "\n",
    "print(\"RMSE = \", RMSE*1000.0, \"meV\")\n",
    "print(\"Sigma test set = \", sigma_test, \" eV\")\n",
    "print(\"%RMSE = \", RMSE/sigma_test*100.0, \" %\")\n",
    "print(\"force RMSE = \", fRMSE, \"eV/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred, s=5.0, c = 'black')\n",
    "lims = [\n",
    "    np.min([np.min(y_pred), np.min(y_test)]),  # min of both axes\n",
    "    np.max([np.max(y_pred), np.max(y_test)]),  # max of both axes\n",
    "]\n",
    "\n",
    "# now plot both limits against eachother\n",
    "plt.plot(lims, lims, 'r-', zorder=0)\n",
    "plt.title(\"correlation plot\")\n",
    "plt.xlabel(\"predicted energies [eV]\")\n",
    "plt.ylabel(\"reference energies [eV]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MD simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use the fitted model to perform a simple NVT simulation at $\\text{T} = 250\\,$K using the i-Pi interface with librascal. For that we will use a communication socket run by i-Pi, which basically outputs a structure produced by the MD and gives it in input to Librascal. This will in turn return energies, forces and stresses by means of the `GenericMDCalculator` class of Librascal. \n",
    "\n",
    "The job itself will generate a parent process (i-Pi) which contains information of the ensemble, the step needed for the time-integration, the thermostat characteristics, and all other trajectory-related infos. All these information are initially stored in an input.xml file as specified in the i-Pi documentation at https://github.com/cosmo-epfl/i-pi and given as inputs to i-Pi. The Librascal calculator is then launched as a child process and exchanges information with the MD driver.  \n",
    "\n",
    "The Librascal driver in i-Pi needs some input parameters, that can be given directly in the command line when the driver is called. To check the needed information, just use the --help option when calling it, as shown below.\n",
    "\n",
    "NOTE: in what follows, it is assumed that your i-Pi and Librascal folders lie in a common directory. Check and modify the path below when defining IPI if this is not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the XML file in the `i-PI/zundel` folder to quickly read the relevant information about the MD settings. Importantly, the `<properties>` tag gives you information about the physical properties that are printed out by i-PI. This simulation gives as output a .out file containing the time-evolution of all the relevant physical quantities, while the .xc.xyz file contains the full trajectory, which you can later visualize with VMD. The file `h5o2+.xyz` is both used by i-PI as a starting configuration of the trajectory and by Librascal as a template to load information about chemical species and number of atoms per species of the system.\n",
    "\n",
    "As the simulation evolves you can plot some interesting physical properties, for instance the MD kinetic energy, the total potential energy and the pressure, and check for thermalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example script is provided to launch a MD simulation with i-PI and the model that has just been fitted `zundel_model.json` (note that you may need to edit some variables in the script, or set `$PATH` and `$PYTHONPATH` appropriately, in order to make it work on your system):\n",
    "\n",
    "```bash\n",
    "bash ./run.sh\n",
    "```\n",
    "\n",
    "and the simulation can be monitored with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the results of the simulations\n",
    "try:\n",
    "    f = open('zundel.out', 'r')\n",
    "except FileNotFoundError:\n",
    "    f = open('zundel.example.out', 'r')\n",
    "lines=f.readlines()[13:]\n",
    "\n",
    "Nframes = len(lines)\n",
    "steps = np.zeros((Nframes, 1), dtype=float)\n",
    "times = np.zeros((Nframes, 1), dtype=float)\n",
    "KE = np.zeros((Nframes, 1), dtype=float)\n",
    "PE = np.zeros((Nframes, 1), dtype=float)\n",
    "Pressure = np.zeros((Nframes, 1), dtype=float)\n",
    "\n",
    "i = 0\n",
    "for x in lines: \n",
    "    steps[i] = float(x.split()[0])\n",
    "    times[i] = float(x.split()[1])\n",
    "    KE[i] = float(x.split()[4])\n",
    "    PE[i] = float(x.split()[5])\n",
    "    Pressure[i] = float(x.split()[6])\n",
    "    i += 1\n",
    "\n",
    "f, axs = plt.subplots(2, 1, sharex=True)\n",
    "f.subplots_adjust(hspace=0)\n",
    "f.suptitle('T = 250 K', fontweight='bold')\n",
    "\n",
    "axs[0].plot(times, (KE - KE[0]), linewidth = 1.5, label = r'$\\Delta PE$')\n",
    "axs[0].plot(times, (PE - PE[0]), linewidth = 1.5, label = r'$\\Delta KE$')\n",
    "axs[0].set(ylabel= 'Energy [eV]')\n",
    "axs[0].set_ylim(-0.5, 0.5)\n",
    "axs[0].legend(ncol = 2, mode='expand', prop={'size': 12}, loc=\"lower center\")\n",
    "\n",
    "axs[1].plot(times, Pressure, linewidth = 1.5, color = 'red')\n",
    "axs[1].set(ylabel= 'MD Pressure [bar]')\n",
    "axs[1].set(xlabel= 'time [ps]')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
